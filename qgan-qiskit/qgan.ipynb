{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import *\n",
    "from qiskit.visualization import *\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "from qiskit.algorithms.optimizers import COBYLA,SLSQP,SPSA, ADAM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "qasmsim = BasicAer.get_backend(\"qasm_simulator\")\n",
    "statevecsim = BasicAer.get_backend(\"statevector_simulator\")\n",
    "TOTAL_COUNTS= 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "\n",
    "    def __init__(self, feature_vector, d_params, num_ansatz_reps):\n",
    "\n",
    "        self.feature_vector = feature_vector\n",
    "        self.d_params = d_params\n",
    "        self.num_ansatz_reps = num_ansatz_reps\n",
    "\n",
    "\n",
    "    def d_distribution(self):\n",
    "\n",
    "        num_qubits = len(self.feature_vector)\n",
    "        qc = QuantumCircuit(num_qubits,1)\n",
    "\n",
    "        #---------- QUANTUM FEATURE MAP ------------ #\n",
    "        feature_vector = self.feature_vector\n",
    "        for qubit, variable in enumerate(feature_vector):\n",
    "            qc.ry(variable, qubit)\n",
    "        \n",
    "        # -------------- ANSATZ --------------- #\n",
    "        qc.barrier()\n",
    "        ansatz = RealAmplitudes(num_qubits=num_qubits, \n",
    "                                reps=self.num_ansatz_reps,\n",
    "                                ).assign_parameters(self.d_params)\n",
    "\n",
    "        # --------------- MEASUREMENT ------------------ #\n",
    "        qc.append(ansatz, list(range(num_qubits)))\n",
    "        qc.measure(len(feature_vector)-1 , 0)\n",
    "        counts = execute(qc, backend=qasmsim, counts=TOTAL_COUNTS).result().get_counts(qc)\n",
    "        return counts\n",
    "\n",
    "    def parity_function(self):\n",
    "\n",
    "        P_d = self.d_distribution()\n",
    "\n",
    "        # ------------ FIXING QISKIT BUGS ------------ #\n",
    "        if '0' not in P_d:\n",
    "            P_d['0']=0\n",
    "        if '1' not in P_d:\n",
    "            P_d['1']=0\n",
    "\n",
    "        # ----------- PARITY FUNCTION <Z> ------------ #\n",
    "        exp_value =  (P_d['0'] - P_d['1'])/TOTAL_COUNTS                   \n",
    "        norm_exp_value = (exp_value + 1)/2\n",
    "\n",
    "        return norm_exp_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, latent_vector, g_params, num_ansatz_reps):\n",
    "\n",
    "        self.latent_vector = latent_vector\n",
    "        self.g_params = g_params\n",
    "        self.num_ansatz_reps = num_ansatz_reps\n",
    "\n",
    "    def g_distribution(self):\n",
    "\n",
    "        num_qubits = len(self.latent_vector)\n",
    "        aux_qubits = 1\n",
    "        total_qubits = num_qubits + aux_qubits\n",
    "        qc = QuantumCircuit(total_qubits, num_qubits)\n",
    "\n",
    "        # -------------------- QUANTUM FEATURE MAP -------------------- #\n",
    "        latent_vector = self.latent_vector\n",
    "        for qubit, variable in enumerate(latent_vector):\n",
    "            qc.ry(variable, qubit)\n",
    "        \n",
    "        # --------------------------- ANSATZ -------------------------- #\n",
    "        qc.barrier()\n",
    "        ansatz = RealAmplitudes(num_qubits=num_qubits, \n",
    "                                reps=self.num_ansatz_reps,\n",
    "                                ).assign_parameters(self.g_params)\n",
    "\n",
    "        # ----------------- GENERATOR DISTRIBUTION ------------------- #\n",
    "        qc.append(ansatz, list(range(total_qubits))[1:])\n",
    "        qc.measure(range(total_qubits)[1:], range(num_qubits))\n",
    "        counts = execute(qc, backend=qasmsim, shots=TOTAL_COUNTS).result().get_counts(qc)\n",
    "\n",
    "        # ------------------ SORTED DISTRIBUTION --------------------- #\n",
    "        strings=['00','01','10','11']\n",
    "\n",
    "        sorted_counts = {}\n",
    "        for string in strings:\n",
    "            if string not in counts:\n",
    "                counts[string]=0\n",
    "            sorted_counts[string] = counts[string]\n",
    "\n",
    "        probs = []\n",
    "        for key, value in sorted_counts.items():\n",
    "            probs.append( (2*np.pi* value) / TOTAL_COUNTS)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing normalized real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'data'))\n",
    "filename = \"irissetosa.data\"\n",
    "data = pd.read_csv(os.path.join(path,filename))\n",
    "label = data.iloc[:, -1]\n",
    "features = data.iloc[:, :-1]\n",
    "\n",
    "x = features.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 2*np.pi))\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "features = pd.DataFrame(x_scaled)\n",
    "data = features.assign(labels = label)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy from Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_crossentropy(disc_params):\n",
    "\n",
    "    # -------------------- DISCRIMINATOR CROSS-ENTROPY --------------------- #\n",
    "    d_loss = 0\n",
    "\n",
    "    for fake_data, real_data in zip(fake_minibatch, real_minibatch):\n",
    "        # ---------------- PASS FAKE DATA ON DISCRIMINATOR ----------------- #\n",
    "        fake_y_estimator = Discriminator(feature_vector = fake_data, \n",
    "                                        d_params = disc_params, \n",
    "                                        num_ansatz_reps = d_num_reps\n",
    "                                        ).parity_function()\n",
    "\n",
    "        # ------------------ DISCRIMINATOR LOSS FOR FAKE ------------------- #            \n",
    "        d_loss_fake = -(np.log(1 - fake_y_estimator))\n",
    "\n",
    "        # ---------------- PASS REAL DATA ON DISCRIMINATOR ----------------- #\n",
    "        real_y_estimator = Discriminator(feature_vector = real_data, \n",
    "                                         d_params = disc_params, \n",
    "                                         num_ansatz_reps = d_num_reps\n",
    "                                        ).parity_function()\n",
    "\n",
    "        # ------------------ DISCRIMINATOR LOSS FOR REAL ------------------- #            \n",
    "        d_loss_real = -(np.log(real_y_estimator))\n",
    "\n",
    "        # -----------------DISCRIMINATOR CROSS ENTROPY --------------------- #\n",
    "        d_loss += d_loss_fake + d_loss_real\n",
    "\n",
    "    d_loss = d_loss/NUM_SAMPLES\n",
    "    return d_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy from Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_crossentropy(g_params):\n",
    "    \n",
    "    # -------------------- GENERATOR CROSS-ENTROPY --------------------- #\n",
    "    g_loss = 0    \n",
    "    for fake_data in fake_minibatch:\n",
    "        # ---------------- PASS FAKE DATA ON DISCRIMINATOR ----------------- #\n",
    "        fake_y_estimator = Discriminator(feature_vector = fake_data, \n",
    "                                         d_params = optimal_params_discriminator, \n",
    "                                         num_ansatz_reps = d_num_reps\n",
    "                                         ).parity_function()\n",
    "\n",
    "        # ------------------------ GENERATOR LOSS -------------------------- #\n",
    "        g_loss = -np.log(fake_y_estimator)\n",
    "\n",
    "    g_loss = g_loss/NUM_SAMPLES\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_entropy(G, X):\n",
    "\n",
    "    \"\"\"Kullback-Leibler divergence\"\"\"\n",
    "\n",
    "    d_kl = 0\n",
    "    for x in X:\n",
    "        for g in G:\n",
    "            #d_kl += sum([(g[i]*np.log(g[i]/x[i]))\n",
    "            d_kl += sum([(g[i]-x[i])\n",
    "                            for i in range(len(g))])\n",
    "\n",
    "    return d_kl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- PARAMETERS OF THE PROBLEMS -------------- # \n",
    "GEN_IT = 1; EPOCHS=100; DISC_IT = 5; NUM_SAMPLES = data.shape[0]\n",
    "g_num_qubits = 2; d_num_qubits = 4; g_num_reps = 4; d_num_reps = 4\n",
    "g_initial_guess = np.random.uniform(0, np.pi, g_num_qubits + g_num_qubits*g_num_reps)\n",
    "d_initial_guess = np.random.uniform(0, np.pi, d_num_qubits + d_num_qubits*d_num_reps)\n",
    "loss_values_discriminator, loss_values_generator, KL_divergence = [], [], []\n",
    "\n",
    "# ---------------- PREPARE REAL MINIBATCH  --------------- #    \n",
    "real_minibatch = []\n",
    "\n",
    "for m in range(NUM_SAMPLES):        \n",
    "    # --------------------- SAMPLE FROM REAL DATA ---------------------- #\n",
    "    real_vector = data.iloc[m, :-1] \n",
    "    real_minibatch.append(real_vector)\n",
    "\n",
    "# ---------------- SETTING THE OPTIMIZERS  --------------- #\n",
    "disc_optimizer = ADAM(maxiter=DISC_IT, tol=1e-06, lr=0.01, beta_1=0.9, beta_2=0.99, noise_factor=1e-08, eps=1e-10)\n",
    "gen_optimizer = ADAM(maxiter=GEN_IT, tol=1e-06, lr=0.01, beta_1=0.9, beta_2=0.99, noise_factor=1e-08, eps=1e-10)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"---------------------------------------------- epoch = {epoch} ----------------------------------------------\")\n",
    "    print( \"-------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # --------------- PREPARE LATENT MINIBATCH  -----------------#\n",
    "    latent_minibatch = []\n",
    "    for m in range(NUM_SAMPLES):        \n",
    "        # -------------------- GENERATE LATENT VECTOR ---------------------- #\n",
    "        latent_vec = np.random.uniform(0, 2*np.pi, g_num_qubits)\n",
    "        #latent_vec = [np.pi*np.random.random(), 2*np.pi*np.random.random()]\n",
    "        latent_minibatch.append(latent_vec)    \n",
    "    # --------------- PREPARE FAKE MINIBATCH  -----------------#\n",
    "    fake_minibatch = []\n",
    "    for m in latent_minibatch:        \n",
    "        # ---------------- PASS LATENT VECTOR ON GENERATOR ----------------- #\n",
    "        generator = Generator(latent_vector = m, \n",
    "                                g_params = g_initial_guess, \n",
    "                                num_ansatz_reps = g_num_reps)\n",
    "        # -------------------- GENERATE FAKE DISTRIBUTION ------------------ #\n",
    "        fake_distr = generator.g_distribution()            \n",
    "        fake_minibatch.append(fake_distr)\n",
    "\n",
    "    \n",
    "    # ------------------ TRAIN DISCRIMINATOR --------------------#\n",
    "    result_d = disc_optimizer.minimize(fun = D_crossentropy,\n",
    "                                       x0 = d_initial_guess)\n",
    "    \n",
    "    optimal_params_discriminator = result_d.x\n",
    "    d_initial_guess = optimal_params_discriminator\n",
    "    loss_values_discriminator.append(result_d.fun)\n",
    "\n",
    "\n",
    "    # ------------------ TRAIN GENERATOR --------------------#\n",
    "    result_g = gen_optimizer.minimize(fun = G_crossentropy,\n",
    "                                       x0 = g_initial_guess)\n",
    "    \n",
    "    optimal_params_generator = result_g.x\n",
    "    g_initial_guess = optimal_params_generator\n",
    "    loss_values_generator.append(result_g.fun)\n",
    "\n",
    "    # ------------------ RELATIVE ENTROPY -------------------#\n",
    "    kl_divergence = relative_entropy(fake_minibatch, real_minibatch)\n",
    "    KL_divergence.append(kl_divergence)\n",
    "\n",
    "\n",
    "    print(f\"Discriminator Loss: {result_d.fun} \\t Generator Loss: {result_g.fun} \\t KL Divergence: {kl_divergence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values_discriminator = loss_values_discriminator/np.linalg.norm(loss_values_discriminator)\n",
    "loss_values_generator = loss_values_generator/np.linalg.norm(loss_values_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_values_discriminator)\n",
    "plt.plot(loss_values_generator)\n",
    "plt.ylabel('Loss function')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(KL_divergence)\n",
    "plt.ylabel('Divergence')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEW_SAMPLES = 50\n",
    "\n",
    "new_samples = []\n",
    "\n",
    "for sample in range(NUM_NEW_SAMPLES):\n",
    "    # ----------------------- NEW LATENT VECTOR ------------------------ #\n",
    "    latent_vec = np.random.uniform(0, 2*np.pi, g_num_qubits)\n",
    "\n",
    "    # ---------------- PASS LATENT VECTOR ON GENERATOR ----------------- #\n",
    "    generator = Generator(latent_vector = latent_vec, \n",
    "                            g_params = optimal_params_generator, \n",
    "                            num_ansatz_reps = g_num_reps)\n",
    "\n",
    "    # -------------------- GENERATE FAKE DISTRIBUTION ------------------ #\n",
    "    fake_distr = generator.g_distribution()            \n",
    "    new_samples.append(fake_distr)\n",
    "\n",
    "print(\"NEW SAMPLES DERIVED FROM QUANTUM GENERATOR: \\n \", np.matrix(new_samples)[:5])\n",
    "\n",
    "\n",
    "new_data = np.array(new_samples)\n",
    "\n",
    "sepal_len = [new_data[i][0] for i in range(len(new_data))]\n",
    "sepal_width = [new_data[i][1] for i in range(len(new_data))]\n",
    "petal_len = [new_data[i][2] for i in range(len(new_data))]\n",
    "petal_width = [new_data[i][3] for i in range(len(new_data))]\n",
    "\n",
    "plt.scatter(sepal_len, petal_len)\n",
    "#plt.scatter(sepal_width, petal_width)\n",
    "\n",
    "plt.title(\"Correlation between variables\")\n",
    "plt.legend([\"SL vs PL\", \"SW vs PW\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.iloc[:, 0].to_numpy(), data.iloc[:, 2].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
